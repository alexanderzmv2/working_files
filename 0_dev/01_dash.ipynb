{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестировочный файл\n",
    "В данном блокноте отрабатываются следующие направления:\n",
    "- Подключение к SQLite базе(ам)\n",
    "- Использование в качестве файла хранения и промежуточного запросного Parquet файла\n",
    "- Разработка визуальных элементов (графиков, индикаторов, чаты?) и таблиц \n",
    "- Тестирование библиотек для работы в качестве отдельного приложения (Dash, Streamlit)\n",
    "- Персонализация и частичный контроль доступа\n",
    "- Разбивка на составляющие (ipynb, py) файлы\n",
    "- Тестирование работы всего приложения (скорость, нагрузка на ЦП и ОЗУ, ошибки и пр.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доработки\n",
    "\n",
    "- Добавить запросы sql из sqlite +\n",
    "- добавить загрузку в parquet ...>>>\n",
    "- добавить проверку наличия бд на диске перед подключением\n",
    "- Настроить через обратные вызовы связь sqlite > parquet > pandas query\n",
    "- Начать добавлять визуализацию plotly\n",
    "- Добавить таблицу и форматированные\n",
    "- Доработать обработчик ошибок, чтобы краткая ошибка выходила на экран, остальное в Лог\n",
    "- Посмотреть оболочку, которая будет выводить приложение в отдельную веб страницу или сделать через jupyter\n",
    "- Карты, письмо от ФД, у них есть сервер с тайтлами!\n",
    "- добавить ролевой доступ и проработать структуру\n",
    "- добавить резервирование БД\n",
    "- добавить файл со статусом, где будет: 1) дата изменения файла, которая будет сравниваться с той что на ресурсе 2) статус базы, желательно её имя т.е например чтобы загрузка из резервной подгружалась\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Набор методов для обращения к SQLite БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПЕРЕНЕСТИ В МЕТОД SQLITE_CN! ФУНКЦИЯ ДЛЯ ПРОВЕРКИ НАЛИЧИЯ БД НА ДИСКЕ\n",
    "def check_db(filename):\n",
    "    return os.path.exists(filename)\n",
    "#ТАКЖЕ ДОБАВИТЬ ФУНКЦИЮ, КОТОРАЯ БУДЕТ ПРОВЕРЯТЬ: 1) ВРЕМЯ ИЗМЕНЕНИЯ ФАЙЛА 2) ЗАНЯТОСТИ ФАЙЛА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем класс, для обращения с БД SQLite\n",
    "class sqlite_cn:\n",
    "#загружаем обязательные модули\n",
    "    import sqlite3 as sql3\n",
    "    import pandas as pd\n",
    "#при вызове сразу осуществляем подключение к БД и присоединенным базам\n",
    "    def __init__(self, db_folder, db_basic, db_attached = []):\n",
    "        try:\n",
    "            global cn #объявляем cn глобальной переменной, чтобы она была видна в остальных методах\n",
    "            err_msg = None\n",
    "            #проверяем существует ли подключение сейчас, если да отключаем\n",
    "            if 'cn' in vars(__builtins__):\n",
    "                if (cn):\n",
    "                    cn.close(); cn = None\n",
    "            #подключение к основной БД\n",
    "            full_path = db_folder + db_basic + '.db'\n",
    "            print('Connecting to: ', full_path)\n",
    "            cn = self.sql3.connect(r'' + full_path)\n",
    "            #соединение баз, соединяем перебором (если список)\n",
    "            if db_attached:\n",
    "                for db in db_attached:\n",
    "                    qr_att = r'ATTACH DATABASE \"' + db_folder + db + '.db' + '\" AS ' + db\n",
    "                    cn.execute(qr_att)\n",
    "                    cn.commit()\n",
    "                    print('DB \"' + db + '\" attached')\n",
    "        except self.sql3.Error as error:\n",
    "            err_msg = error #если ошибка, выводим\n",
    "            print(\"Error: \", err_msg)\n",
    "        finally:\n",
    "            if err_msg == None and (cn): #отписываемся по итогу\n",
    "                print('Succesfully connected')\n",
    "            else:\n",
    "                print(\"Connection failed!\")\n",
    "#создаем функцию отключения, после выполнения отправки данных      \n",
    "    def disconnect_db(self):\n",
    "        try:\n",
    "            err_msg = None\n",
    "            cn.close() #закрываем соединение\n",
    "        except self.sql3.Error as error:\n",
    "            err_msg = error #если ошибка, выводим\n",
    "            print(\"Error: \", err_msg)\n",
    "        finally:\n",
    "            if err_msg == None and (cn): #отписываемся по итогу\n",
    "                print('Connection closed')\n",
    "            else:\n",
    "                print(\"Closing connection failed!\")\n",
    "#вызов отправки\n",
    "    def send (self, file_folder, file_name,  sql_ifex, xlsx_sheets = [], dtypes = {}): #file_type - xlsx, csv; file_folder: местонахождение файла, file - файл, xlxs_sheets - если пусто, то загружаем всё\n",
    "        try:\n",
    "            err_msg = None\n",
    "            full_path = file_folder + file_name #для удобства соединяем сразу путь к файлу\n",
    "            file_type = full_path.split('.')[-1] #выделяем тип файла\n",
    "            if file_type == 'xlsx': #открываем файл и загружаем его в df или в переменную таблицы\n",
    "                with self.pd.ExcelFile(open(r'' + full_path,'rb')) as wb: #открываем эксельный файл и нам оттуда нужны будут только наименования листов\n",
    "                    for sh in wb.sheet_names:\n",
    "                        if xlsx_sheets:\n",
    "                            if not sh in xlsx_sheets:\n",
    "                                continue\n",
    "                        print ('Sending sheet:', sh)\n",
    "                        self.pd.read_excel(wb,sheet_name = sh).to_sql(sh, \n",
    "                                                                       cn, \n",
    "                                                                       if_exists = sql_ifex, \n",
    "                                                                       index = False,\n",
    "                                                                       dtype = dtypes\n",
    "                                                                      )\n",
    "                shs = None #очищаем переменную\n",
    "            elif file_type == 'csv':\n",
    "                #читаем и отправляем\n",
    "                self.pd.read_csv(open(r'' + full_path,'r'), sep=';', decimal='.').to_sql(file_name, #!!! для CSV формата, наименование таблицы будет исходить из названия файла\n",
    "                                                                                    cn, \n",
    "                                                                                    if_exists = ifex, \n",
    "                                                                                    index = False,\n",
    "                                                                                    dtype = dtypes)\n",
    "        except self.sql3.Error as error:\n",
    "            err_msg = error\n",
    "            print(\"Error: \", err_msg)\n",
    "        finally:\n",
    "            if err_msg == None and (cn):\n",
    "                print(\"Data successfully sent!\")\n",
    "            else:\n",
    "                print(\"Sending failed!\")\n",
    "            self.disconnect_db()\n",
    "#вызов получения\n",
    "    def get (self, qr, close_conn = True, **args):\n",
    "        try:\n",
    "            err_msg = None\n",
    "            return self.pd.read_sql_query(qr, cn, **args)  #пилим датасет на части, чтобы экономить ОЗУ\n",
    "        except self.sql3.Error as error:\n",
    "            err_msg = error\n",
    "            print(\"Error: \", err_msg)\n",
    "        finally:\n",
    "            if err_msg == None and (cn):\n",
    "                print(\"Successfull get.\")\n",
    "            else:\n",
    "                print(\"Getting failed!\")\n",
    "            #close conn\n",
    "            if close_conn == True: self.disconnect_db() #по умолчанию закрываем соединение, но если мы знаем, что будет большой массив, то оставляем для итераций\n",
    "#сжатие базы данных\n",
    "    def vac (self):\n",
    "        try:\n",
    "            cn.execute(\"VACUUM\") #сжатие\n",
    "            cn.commit() #подтверждение\n",
    "            print('Vacuum succesfull!')\n",
    "        except:\n",
    "            print('Vacuum failed!')\n",
    "        finally:\n",
    "            self.disconnect_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключение к БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое подключение\n",
    "data_folder = r'*\\sql_db\\05_db_' #папка размещения БД-ек\n",
    "x = sqlite_cn (data_folder, \"basic\", ['dics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отправка данных в SQLite из рабочих книг Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.send(r\"\\\\*\\datasets\\basic\", \"\\\\05_datasets_basic_10-20222.xlsx\", 'replace','ds10')\n",
    "#!!! доработать отправку определенных листов, если указываем ds10, то при переборе доступных листов, он также зацепить ds1 т.к. проходит по маске"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение данных из SQLite и конвертация в Parquet файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#функция для формирования запросов\n",
    "def sql_queries(type_query, q_vars):\n",
    "    if type_query == 'main': #получение основного массива данных\n",
    "        return f\"\"\"WITH data_v as (\n",
    "        SELECT src,\n",
    "        dic_fp.fp_id,\n",
    "        dic_fp.fp_value,\n",
    "        dic_fp.aur_kv,\n",
    "        dic_fp.upr_kratko,\n",
    "        dic_fp.upr_polnoe,\n",
    "        dic_fp.gr_zatrat,\n",
    "        dic_fp.podgr_zatrat,\n",
    "        dic_fp.gr_1,\n",
    "        dic_fp.gr_2,\n",
    "        dic_fp.use_main as fp_use_main,\n",
    "        dic_fp.use_re as fp_use_re,\n",
    "        dic_mir.mir_id,\n",
    "        dic_mir.mir_value,\n",
    "        dic_mir.use_main as mir_use_main,\n",
    "        dic_pfm.pfm_id,\n",
    "        dic_pfm.pfm_value,\n",
    "        dic_pfm.tip_seti,\n",
    "        dic_pfm.go_filial,\n",
    "        dic_pfm.filial,\n",
    "        dic_pfm.pap,\n",
    "        dic_pfm.region,\n",
    "        dic_pfm.use_main as pfm_use_main,\n",
    "        dic_proj.proj_id,\n",
    "        dic_proj.proj_value,\n",
    "        IFNULL(dic_proj.napravlenie_zatrat,'Текущая деятельность') as napravlenie_zatrat,\n",
    "        IFNULL(dic_proj.use_main,'Да') as proj_use_main,\n",
    "        IFNULL(dic_proj.use_re,'Да') as proj_use_re,\n",
    "        date, \n",
    "        sum as value\n",
    "        FROM (((((\n",
    "        SELECT '{q_vars[4]}' AS src, * FROM {q_vars[0]} UNION ALL\n",
    "        SELECT '{q_vars[5]}' AS src, * FROM {q_vars[1]} UNION ALL\n",
    "        SELECT '{q_vars[6]}' AS src, * FROM {q_vars[2]} UNION ALL\n",
    "        SELECT '{q_vars[7]}' AS src, * FROM {q_vars[3]}) d\n",
    "        LEFT JOIN dic_fp ON d.fp_id = dic_fp.fp_id)\n",
    "        LEFT JOIN dic_mir ON d.mir_id = dic_mir.mir_id)\n",
    "        LEFT JOIN dic_pfm ON d.pfm_id = dic_pfm.pfm_id)\n",
    "        LEFT JOIN dic_proj ON d.proj_id = dic_proj.proj_id))\n",
    "        SELECT * FROM data_v\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запрашиваем таблицу из SQLite с чанками и сохраняем их в 1 файл Паркет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "#запрашиваемые таблицы\n",
    "src_vars = {'ds2':'Факт 22','ds3':'Факт 21','ds5':'Прогноз 22','ds6':'Бюджет 22',}\n",
    "src_q = list(src_vars.keys()) + list(src_vars.values())\n",
    "#получаем датасет и сохраняем его в паркет\n",
    "chunked_sql_data = x.get(sql_queries('main', src_q), False, chunksize=30000)\n",
    "#перебираем чанки и сохраняем\n",
    "first_itr = True #признак первой итерации\n",
    "for data_chunk in chunked_sql_data: #перебираем чанки\n",
    "    #создаем файл паркет\n",
    "    table = pa.Table.from_pandas(data_chunk)\n",
    "    if first_itr == True: #если первая итерация то сохраняем схему и первые данные\n",
    "        pqwriter = pq.ParquetWriter(r'\\\\*s\\datasets\\prqt_db\\05_main.parquet', table.schema) \n",
    "        first_itr = False\n",
    "    #запись с добавлением в существующий файл\n",
    "    pqwriter.write_table(table) \n",
    "    #pa.parquet.write_to_dataset(table , root_path='sample.parquet')\n",
    "if pqwriter: #закрываем файл записи\n",
    "    pqwriter.close()\n",
    "#закрываем соединение SQLite\n",
    "sqlite_cn.disconnect_db(sqlite_cn)\n",
    "\n",
    "#получить полную сразу\n",
    "#not_chunked_sql_data = x.get(sql_queries('main', src_q))\n",
    "#not_chunked_sql_data.to_parquet('csv_snap2.parquet',compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение данных из Parquet файла до выполнения аггрегации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#получаем необходимый набор данных\n",
    "df = pd.read_parquet(r'*\\05_main.parquet', \n",
    "                     engine = 'pyarrow',\n",
    "                     filters=[\n",
    "                         [('upr_polnoe','==','Управление МТО'),('aur_kv','==','АУР'), ('filial', 'in', ['ГО','СЗФО']), ('napravlenie_zatrat', '=', 'Текущая деятельность')]\n",
    "                     ],\n",
    "                     columns = ['src','date','filial','value']\n",
    "                    )\n",
    "#обрабатываем столбец с датой, добавляем новый с кварталом\n",
    "df['date_q'] = pd.to_datetime(df2.date).dt.to_period('Q').dt.strftime('Q%q').astype(str)\n",
    "#делаем предварительную группировку по значениям\n",
    "dt = df2.query('filial in [\"ГО\"]').groupby(['src','filial','date_q']).agg('sum','value').reset_index().sort_values('date_q')\n",
    "#добавляем столбец по значениям в млн.р\n",
    "dt['val_th'] = (dt['value']/1000000).round(2)\n",
    "#группируем и переворачиваем столбец по источникам\n",
    "dt_t = dt.groupby(['src','filial','date_q']).agg({'val_th':'sum'}).unstack('src').reset_index().droplevel(0, axis=1)\n",
    "#переименовываем столбцы\n",
    "dt_t.columns = ['filial','date','Бюджет 22','Прогноз 22','Факт 21','Факт 22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение графиков и индикаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#тест! Подготовить макеты под построение графиков, в случае типизации, вывести в класс с вызовом через методы\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "layout = go.Layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=300,\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "#    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    margin=go.layout.Margin(\n",
    "        l=10,\n",
    "        r=10,\n",
    "        b=5,\n",
    "        t=25,\n",
    "        pad = 1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(layout = layout)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=dt_t['date'], y=dt_t['Факт 22'],\n",
    "                         name='Факт 22',\n",
    "                         line=dict(dash = 'solid', color = \"#1e90ff\", width = 3),\n",
    "                         text=dt_t['Факт 22'],textposition=\"bottom center\",\n",
    "                         ))\n",
    "fig.add_trace(go.Scatter(x=dt_t['date'], y=dt_t['Факт 21'],\n",
    "                         name='Факт 21',\n",
    "                         line=dict(dash = 'solid', color = \"grey\", width = 3),\n",
    "                         text=dt_t['Факт 21'],textposition=\"bottom center\",\n",
    "                         ))\n",
    "fig.add_trace(go.Scatter(x=dt_t['date'], y=dt_t['Бюджет 22'],\n",
    "                         name='Бюджет', \n",
    "                         line=dict(dash = 'dot', color = '#1034a6', width = 2),\n",
    "                         text=dt_t['Бюджет 22'],textposition=\"top center\",\n",
    "                         ))\n",
    "fig.add_trace(go.Scatter(x=dt_t['date'], y=dt_t['Прогноз 22'],\n",
    "                         name='Прогноз',\n",
    "                         line=dict(dash = 'dash', color = '#fdbe02', width = 2),\n",
    "                         text=list(dt_t['Прогноз 22']),textposition=\"bottom center\",\n",
    "                         ))\n",
    "\n",
    "fig.update_yaxes(visible=False)\n",
    "fig.update_layout()\n",
    "fig.update_traces(mode='lines+markers+text', hoverinfo=\"y+z+name\", selector=dict(type='scatter'), line_shape='spline')\n",
    "\n",
    "fig.update_xaxes(\n",
    "    showspikes=True,\n",
    "    spikecolor=\"#1e90ff\",\n",
    "    spikesnap=\"data\",\n",
    "    spikemode=\"toaxis\",\n",
    "    spikedash=\"solid\",\n",
    "    spikethickness = 1\n",
    ")\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightPink')\n",
    "fig.update_yaxes(visible=True, showgrid=True, gridwidth=1, gridcolor='LightPink', showticklabels = False)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
